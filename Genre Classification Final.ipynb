{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pts = 30000\n",
    "mdat = pd.read_csv('cleanCMURevenueAdjusted.tsv', delimiter='\\t',encoding='utf-8-sig')\n",
    "plotsum = mdat[mdat.columns[7]]\n",
    "mdat[\"Genres\"] = mdat.apply(lambda row: eval(row[\"Genres\"]), axis=1)\n",
    "#g3 = mdat[\"Genres\"]\n",
    "gen = mdat[\"Genres\"]\n",
    "titles = mdat[\"Name\"]\n",
    "mdat.head()\n",
    "\n",
    "#Split\n",
    "Train2, Test2 = train_test_split(mdat, test_size=0.3)\n",
    "#Get plot data\n",
    "TrainPlot = Train2[mdat.columns[7]]\n",
    "TestPlot = Test2[mdat.columns[7]]\n",
    "#Get genre data\n",
    "TrainGen = Train2[\"Genres\"]\n",
    "TestGen = Test2[\"Genres\"]\n",
    "AllGen = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tf-idf the training plots\n",
    "TrainVec = TfidfVectorizer(max_features = 3000, stop_words = stopwords.words('english'))\n",
    "TrainPlotsVec = TrainVec.fit_transform(TrainPlot)\n",
    "#fit the test plots to this tf-idf\n",
    "TestPlotsVec =  TrainVec.transform(TestPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the points with genre lists that include comedy\n",
    "comAll = np.zeros((len(AllGen),))\n",
    "for jj in range(len(AllGen)):\n",
    "    if any(x == 'Comedy' for x in AllGen.iloc[jj]):\n",
    "        comAll[jj] = 1\n",
    "\n",
    "#Find the points with genre lists that include comedy\n",
    "comTest = np.zeros((len(TestGen),))\n",
    "for jj in range(len(TestGen)):\n",
    "    if any(x == 'Comedy' for x in TestGen.iloc[jj]):\n",
    "        comTest[jj] = 1\n",
    "        \n",
    "comTrain = np.zeros((len(TrainGen),))\n",
    "for jj in range(len(TrainGen)):\n",
    "    if any(x == 'Comedy' for x in TrainGen.iloc[jj]):\n",
    "        comTrain[jj] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the points with genre lists that include horror or thriller\n",
    "horTrain = np.zeros((len(TrainGen),))\n",
    "for jj in range(len(TrainGen)):\n",
    "    if any(x == 'Horror' or x == 'Thriller' for x in TrainGen.iloc[jj]):\n",
    "        horTrain[jj] = 1\n",
    "        \n",
    "horTest = np.zeros((len(TestGen),))\n",
    "for jj in range(len(TestGen)):\n",
    "    if any(x == 'Horror' or x == 'Thriller' for x in TestGen.iloc[jj]):\n",
    "        horTest[jj] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765676828305165\n"
     ]
    }
   ],
   "source": [
    "#GBC classify comedy\n",
    "abc = GradientBoostingClassifier()#n_estimators = 5, learning_rate = 1.0, max_depth = 1, random_state = 0 )\n",
    "m = abc.fit(TrainPlotsVec , comTrain)\n",
    "score = m.score(TestPlotsVec, comTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(N, learnrate, acc)\n",
    "(600, 1, 0.7691) \n",
    "(400, 1, 0.7752)\n",
    "(200, 1, 0.7728)\n",
    "(400, 1.2, 0.7613)\n",
    "(400, 0.8, 0.7783)\n",
    "(400, 0.6, 0.78107)\n",
    "(600, 0.4, 0.78328)\n",
    "(800, 0.3, 0.7835)\n",
    "(1000, 0.3, 0.7821)\n",
    "(1000, 0.4, 0.78344)\n",
    "(800, 0.4, 0.78249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7828147212130785\n"
     ]
    }
   ],
   "source": [
    "#Adaboost classify comedy\n",
    "AB_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME.R\",n_estimators=800, learning_rate = 0.3)\n",
    "ABCL = AB_clf.fit(TrainPlotsVec , comTrain)\n",
    "score = ABCL.score(TestPlotsVec, comTest)\n",
    "\n",
    "#abc = GradientBoostingClassifier()#n_estimators = 5, learning_rate = 1.0, max_depth = 1, random_state = 0 )\n",
    "#m = abc.fit(TrainPlotsVec , comTrain)\n",
    "#score = m.score(TestPlotsVec, comTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7475122413520771"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% of points that are not comedy\n",
    "1-sum(comTest)/len(comTest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8429158110882957\n"
     ]
    }
   ],
   "source": [
    "#GBC classify horror\n",
    "abc = GradientBoostingClassifier()#n_estimators = 5, learning_rate = 1.0, max_depth = 1, random_state = 0 )\n",
    "m = abc.fit(TrainPlotsVec , horTrain)\n",
    "horscore = m.score(TestPlotsVec, horTest)\n",
    "print(horscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571315747907123\n"
     ]
    }
   ],
   "source": [
    "AB_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME.R\",n_estimators=800, learning_rate = 0.3)\n",
    "ABCL = AB_clf.fit(TrainPlotsVec , horTrain)\n",
    "score = ABCL.score(TestPlotsVec, horTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212130785026062\n"
     ]
    }
   ],
   "source": [
    "BB_clf = BernoulliNB(alpha=0.00001)\n",
    "BBCL = BB_clf.fit(TrainPlotsVec , horTrain)\n",
    "score = BBCL.score(TestPlotsVec, horTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8386510819775707\n"
     ]
    }
   ],
   "source": [
    "BB_clf = MultinomialNB(alpha = 0.000001)\n",
    "BBCL = BB_clf.fit(TrainPlotsVec , horTrain)\n",
    "score = BBCL.score(TestPlotsVec, horTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(N, learnrate, acc)\n",
    "(400, 1, 0.8428) \n",
    "(500, 1, 0.84307)\n",
    "(600, 1, 0.8425)\n",
    "(500, 0.8, 0.849549)\n",
    "(500, 0.6, 0.85373)\n",
    "(500, 0.4, 0.85318)\n",
    "(600, 0.4, 0.85405)\n",
    "(800, 0.2, 0.850)\n",
    "(1000, 0.3, 0.8533)\n",
    "(1000, 0.4, 0.85547)\n",
    "(1200, 0.4, 0.85642)\n",
    "(1400, 0.4, 0.85499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7863686621386827"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-sum(horTest)/len(horTest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
